# Notes

## Regarding the huggingface cache management
https://huggingface.co/docs/huggingface_hub/en/guides/manage-cache

## Optimization vis.
Notebooks
https://distill.pub/
Momentum for opt.
https://distill.pub/2017/momentum/

## Class note
csc2541_2022
https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2022/

csc413
https://csc413-w25.notion.site/CSC413-2516-Neural-Networks-and-Deep-Learning-152946d7006c803087fddccbbdd36a86

## GAN visualization
https://poloclub.github.io/ganlab/

## Example Colab Notebooks

Here are the presentations and Colab notebooks from last year. You can use these as examples, and you may also find it interesting to read them to get more insight into the papers. Note that, due to the size of the class, we're only doing the Colab notebooks this year, not the presentations.

Presenters	Paper	Materials
1. Neural tangent kernel: Convergence and generalization in neural networks. NeurIPS 2018.	[Slides] [Colab] \\
2. The deep bootstrap framework: Good online learners are good offline generalizers. ICLR 2021.	[Slides] [Colab]
3. OpenAI Dota Team. An empirical model of large-batch training. 2018.	[Slides] [Colab]
4. The implicit and explicit regularization effects of dropout. ICML 2020.	[Slides] [Colab]
5. Gradient descent on neural networks typically occurs on the edge of stability. ICLR 2021.	[Slides] [Colab]
6. On the importance of initialization and momentum in deep learning	[Slides] [Colab 1] [Colab 2]
7. A mathematical theory of semantic development in deep neural networks	[Slides] [Colab]
8. Hamiltonian descent methods.	[Slides] [Colab]
9. Gradient-based Hyperparameter Optimization through Reversible Learning.	[Slides] [Colab]
10. Understanding black-box predictions via influence functions.	[Slides] [Colab]
11. Deep equilibrium models	[Slides] [Colab]
12. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks	[Slides] [Colab]
