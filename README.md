# Notes

## Regarding the huggingface cache management
https://huggingface.co/docs/huggingface_hub/en/guides/manage-cache

## Optimization vis.
Notebooks
https://distill.pub/
Momentum for opt.
https://distill.pub/2017/momentum/

## Class note
csc2541_2022
https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2022/

## GAN visualization
https://poloclub.github.io/ganlab/

## Example Colab Notebooks

Here are the presentations and Colab notebooks from last year. You can use these as examples, and you may also find it interesting to read them to get more insight into the papers. Note that, due to the size of the class, we're only doing the Colab notebooks this year, not the presentations.

Presenters	Paper	Materials
Kimia Hamidieh, Nathan Ng, Haoran Zhang	A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in neural networks. NeurIPS 2018.	[Slides] [Colab]
Chris Zhang, Dami Choi, Anqi (Joyce) Yang	P. Nakkiran, B. Neyshabur, and H. Sedghi. The deep bootstrap framework: Good online learners are good offline generalizers. ICLR 2021.	[Slides] [Colab]
Borys Bryndak, Sergio Casas, and Sean Segal	S. McCandish, J. Kaplan, D. Amodei, and the OpenAI Dota Team. An empirical model of large-batch training. 2018.	[Slides] [Colab]
Kelvin Wong, Siva Manivasagam, and Amanjit Singh Kainth	C. Wei, S. Kakade, and T. Ma. The implicit and explicit regularization effects of dropout. ICML 2020.	[Slides] [Colab]
Honghua Dong and Tianxing Li	J. Cohen, S. Kaur, Y. Li, J. Z. Kolter, and A. Talwalkar. Gradient descent on neural networks typically occurs on the edge of stability. ICLR 2021.	[Slides] [Colab]
Yuwen Xiong, Andrew Liao, and Jingkang Wang	I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and momentum in deep learning	[Slides] [Colab 1] [Colab 2]
Jenny Bao, Sheldon Huang, and Skylar Hao	A. M. Saxe, J. L. McClelland, and S. Ganguli. A mathematical theory of semantic development in deep neural networks	[Slides] [Colab]
James Tu, Yangjun Ruan, and Jonah Philion	C. Maddison, D. Paulin, Y.-W. Teh, B. O'Donoghue, and A. Doucet. Hamiltonian descent methods.	[Slides] [Colab]
Haoping Xu, Zhihuan Yu, and Jingcheng Niu	D. Maclaurin, D. Duvenaud, and R. P. Adams. Gradient-based Hyperparameter Optimization through Reversible Learning.	[Slides] [Colab]
Alex Adam, Keiran Paster, and Jenny (Jingyi) Liu	P.-W. Koh and P. Liang. Understanding black-box predictions via influence functions.	[Slides] [Colab]
Alex Wang, Gary Leung, and Sasha Doubov	S. Bai, J. Z. Kolter, and V. Koltun. Deep equilibrium models	[Slides] [Colab]
Shihao Ma, Yichun Zhang, and Zilun Zhang	C. Finn, P. Abbeel, and S. Levine. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks	[Slides] [Colab]
